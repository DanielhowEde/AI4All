//! Web crawler backend
//!
//! Implements the WebCrawl task type: fetches pages via HTTP, extracts clean text
//! using CSS selectors, follows links up to a configurable BFS depth, and optionally
//! generates vector embeddings via the configured OpenAI-compatible endpoint.

use std::collections::{HashMap, HashSet, VecDeque};
use std::path::Path;
use std::time::{Duration, Instant};

use async_trait::async_trait;
use sha2::{Digest, Sha256};
use tracing::{debug, info, warn};
use url::Url;

use crate::backend::traits::{
    BackendCapabilities, BackendHealth, InferenceBackend, ResourceUsage,
};
use crate::config::{CrawlerSettings, OpenAiSettings};
use crate::error::{Error, Result};
use crate::types::{
    CrawledPage, LoadedModelInfo, ModelSpec, TaskType, TextCompletionInput,
    TextCompletionOutput, WebCrawlInput, WebCrawlOutput,
};

// ─────────────────────────────────────────────────────────────────
// CrawlerBackend
// ─────────────────────────────────────────────────────────────────

/// Backend that handles WEB_CRAWL tasks.
///
/// Uses `reqwest` for HTTP and `scraper` for HTML parsing.
/// Embeddings (optional) are generated by calling the OpenAI-compatible
/// `/embeddings` endpoint directly — no circular registry dependency.
pub struct CrawlerBackend {
    http_client: reqwest::Client,
    openai_base_url: String,
    openai_api_key: String,
    embed_model: String,
    rate_limit_ms: u64,
    respect_robots: bool,
    user_agent: String,
}

impl CrawlerBackend {
    /// Create a new CrawlerBackend from config sections.
    pub fn new(crawler: &CrawlerSettings, openai: &OpenAiSettings) -> Self {
        let user_agent = if crawler.user_agent.is_empty() {
            format!("AI4All/{}", env!("CARGO_PKG_VERSION"))
        } else {
            crawler.user_agent.clone()
        };

        let http_client = reqwest::Client::builder()
            .timeout(Duration::from_secs(20))
            .user_agent(&user_agent)
            .build()
            .unwrap_or_default();

        Self {
            http_client,
            openai_base_url: openai.base_url.clone(),
            openai_api_key: openai.api_key.clone(),
            embed_model: openai.default_model.clone(),
            rate_limit_ms: crawler.rate_limit_ms,
            respect_robots: crawler.respect_robots,
            user_agent,
        }
    }

    // ── Internal helpers ──────────────────────────────────────────

    /// Fetch a URL, parse HTML, return (title, body_text, outbound_links).
    async fn fetch_page(&self, url: &str) -> Result<(Option<String>, String, Vec<String>)> {
        let resp = self
            .http_client
            .get(url)
            .send()
            .await
            .map_err(|e| Error::Execution(format!("HTTP fetch failed for {}: {}", url, e)))?;

        if !resp.status().is_success() {
            return Err(Error::Execution(format!(
                "HTTP {} fetching {}",
                resp.status(),
                url
            )));
        }

        // Only parse text/html content
        let content_type = resp
            .headers()
            .get(reqwest::header::CONTENT_TYPE)
            .and_then(|v| v.to_str().ok())
            .unwrap_or("")
            .to_string();
        if !content_type.contains("html") {
            return Err(Error::Execution(format!(
                "Non-HTML content-type '{}' at {}",
                content_type, url
            )));
        }

        let html = resp
            .text()
            .await
            .map_err(|e| Error::Execution(format!("Failed to read body of {}: {}", url, e)))?;

        let document = scraper::Html::parse_document(&html);

        // Title
        let title_sel = scraper::Selector::parse("title").expect("valid selector");
        let title = document
            .select(&title_sel)
            .next()
            .map(|e| e.text().collect::<Vec<_>>().join(""))
            .map(|t| t.trim().to_string())
            .filter(|t| !t.is_empty());

        // Body text — target semantic elements, skipping script/style implicitly
        let text_sel = scraper::Selector::parse(
            "p, h1, h2, h3, h4, h5, h6, li, article, section, main, \
             td, th, blockquote, pre, figcaption, dt, dd",
        )
        .expect("valid selector");
        let text: String = document
            .select(&text_sel)
            .flat_map(|e| e.text())
            .map(|t| t.trim())
            .filter(|t| !t.is_empty())
            .collect::<Vec<_>>()
            .join("\n");

        // Outbound links — resolve relative URLs against the page base
        let base = Url::parse(url).ok();
        let link_sel = scraper::Selector::parse("a[href]").expect("valid selector");
        let links: Vec<String> = document
            .select(&link_sel)
            .filter_map(|e| e.value().attr("href"))
            .filter_map(|href| {
                if let Some(ref b) = base {
                    b.join(href).ok().map(|u| u.to_string())
                } else {
                    Url::parse(href).ok().map(|u| u.to_string())
                }
            })
            .filter(|u| u.starts_with("http://") || u.starts_with("https://"))
            .collect();

        Ok((title, text, links))
    }

    /// Check robots.txt for the URL.  Returns `true` if crawling is allowed.
    ///
    /// `robots_cache` maps host → list of disallowed path prefixes.
    async fn is_robots_allowed(
        &self,
        robots_cache: &mut HashMap<String, Vec<String>>,
        url: &str,
    ) -> bool {
        if !self.respect_robots {
            return true;
        }
        let parsed = match Url::parse(url) {
            Ok(u) => u,
            Err(_) => return true,
        };
        let host = match parsed.host_str() {
            Some(h) => h.to_string(),
            None => return true,
        };
        let path = parsed.path().to_string();

        if !robots_cache.contains_key(&host) {
            let robots_url = format!("{}://{}/robots.txt", parsed.scheme(), host);
            let disallowed = self.fetch_robots_disallowed(&robots_url).await;
            robots_cache.insert(host.clone(), disallowed);
        }

        let disallowed = robots_cache.get(&host).unwrap();
        !disallowed
            .iter()
            .any(|prefix| path.starts_with(prefix.as_str()))
    }

    /// Fetch and parse robots.txt, returning a list of Disallow path prefixes
    /// that apply to `User-agent: *` or `User-agent: AI4All`.
    async fn fetch_robots_disallowed(&self, robots_url: &str) -> Vec<String> {
        let resp = match self
            .http_client
            .get(robots_url)
            .timeout(Duration::from_secs(5))
            .send()
            .await
        {
            Ok(r) if r.status().is_success() => r,
            _ => return vec![],
        };
        let text = match resp.text().await {
            Ok(t) => t,
            Err(_) => return vec![],
        };

        let mut in_relevant = false;
        let mut disallowed = Vec::new();
        for raw in text.lines() {
            let line = raw.trim();
            if line.starts_with("User-agent:") {
                let agent = line["User-agent:".len()..].trim();
                in_relevant = agent == "*" || agent.eq_ignore_ascii_case("AI4All");
            } else if in_relevant && line.starts_with("Disallow:") {
                let path = line["Disallow:".len()..].trim();
                if !path.is_empty() {
                    disallowed.push(path.to_string());
                }
            }
        }
        disallowed
    }

    /// Call the OpenAI-compatible /embeddings endpoint and return the vector.
    async fn embed_text(&self, text: &str) -> Option<Vec<f32>> {
        if self.openai_base_url.is_empty() {
            return None;
        }
        // Truncate to ~8 000 chars to stay within typical token limits
        let snippet = if text.len() > 8000 { &text[..8000] } else { text };

        #[derive(serde::Deserialize)]
        struct EmbedData {
            embedding: Vec<f32>,
        }
        #[derive(serde::Deserialize)]
        struct EmbedResponse {
            data: Vec<EmbedData>,
        }

        let resp = self
            .http_client
            .post(format!("{}/embeddings", self.openai_base_url))
            .bearer_auth(&self.openai_api_key)
            .json(&serde_json::json!({
                "model": self.embed_model,
                "input": snippet,
            }))
            .send()
            .await
            .ok()?;

        let result: EmbedResponse = resp.json().await.ok()?;
        result.data.into_iter().next().map(|d| d.embedding)
    }
}

// ─────────────────────────────────────────────────────────────────
// InferenceBackend implementation
// ─────────────────────────────────────────────────────────────────

#[async_trait]
impl InferenceBackend for CrawlerBackend {
    fn name(&self) -> &'static str {
        "crawler"
    }

    fn capabilities(&self) -> BackendCapabilities {
        BackendCapabilities {
            name: "crawler",
            supported_tasks: vec![TaskType::WebCrawl],
            supports_training: false,
            supports_streaming: false,
            max_context_length: 0,
            max_batch_size: 1,
            gpu_available: false,
            gpu_device: None,
        }
    }

    async fn health_check(&self) -> Result<BackendHealth> {
        Ok(BackendHealth::default())
    }

    fn resource_usage(&self) -> ResourceUsage {
        ResourceUsage::default()
    }

    async fn load_model(&mut self, _spec: &ModelSpec) -> Result<LoadedModelInfo> {
        Err(Error::NotSupported(
            "Crawler backend does not load models".to_string(),
        ))
    }

    async fn load_model_from_path(&mut self, _path: &Path) -> Result<LoadedModelInfo> {
        Err(Error::NotSupported(
            "Crawler backend does not load models".to_string(),
        ))
    }

    async fn unload_model(&mut self) -> Result<()> {
        Ok(())
    }

    fn loaded_model(&self) -> Option<&LoadedModelInfo> {
        None
    }

    async fn text_completion(
        &self,
        _input: TextCompletionInput,
    ) -> Result<TextCompletionOutput> {
        Err(Error::NotSupported(
            "Crawler backend only handles WebCrawl tasks".to_string(),
        ))
    }

    async fn web_crawl(&self, input: WebCrawlInput) -> Result<WebCrawlOutput> {
        let max_pages = input.max_pages.max(1) as usize;

        let mut pages: Vec<CrawledPage> = Vec::new();
        let mut errors: Vec<String> = Vec::new();
        let mut visited: HashSet<String> = HashSet::new();
        let mut robots_cache: HashMap<String, Vec<String>> = HashMap::new();
        let mut rate_tracker: HashMap<String, Instant> = HashMap::new();
        // BFS queue: (url, depth)
        let mut queue: VecDeque<(String, u32)> = VecDeque::new();
        queue.push_back((input.url.clone(), 0));

        while let Some((url, depth)) = queue.pop_front() {
            if pages.len() >= max_pages {
                break;
            }
            if visited.contains(&url) {
                continue;
            }
            visited.insert(url.clone());

            // robots.txt
            if !self.is_robots_allowed(&mut robots_cache, &url).await {
                debug!(url = %url, "Skipped: disallowed by robots.txt");
                continue;
            }

            // Per-domain rate limiting
            if let Ok(parsed) = Url::parse(&url) {
                if let Some(host) = parsed.host_str().map(|h| h.to_string()) {
                    if let Some(last) = rate_tracker.get(&host).copied() {
                        let elapsed_ms = last.elapsed().as_millis() as u64;
                        if elapsed_ms < self.rate_limit_ms {
                            let wait = self.rate_limit_ms - elapsed_ms;
                            tokio::time::sleep(Duration::from_millis(wait)).await;
                        }
                    }
                    rate_tracker.insert(host, Instant::now());
                }
            }

            info!(url = %url, depth = depth, "Crawling page");

            let (title, text, raw_links) = match self.fetch_page(&url).await {
                Ok(r) => r,
                Err(e) => {
                    warn!(url = %url, error = %e, "Failed to fetch");
                    errors.push(format!("{}: {}", url, e));
                    continue;
                }
            };

            if text.is_empty() {
                debug!(url = %url, "Skipping empty page");
                continue;
            }

            // Content hash for dedup
            let mut hasher = Sha256::new();
            hasher.update(text.as_bytes());
            let content_hash = hex::encode(hasher.finalize());

            // Embeddings (optional)
            let embedding = if input.generate_embeddings {
                self.embed_text(&text).await
            } else {
                None
            };

            // BFS link expansion
            let outbound: Vec<String> = if depth < input.max_depth {
                raw_links
                    .iter()
                    .filter(|link| !visited.contains(*link))
                    .filter(|link| {
                        if input.allowed_domains.is_empty() {
                            return true;
                        }
                        Url::parse(link)
                            .ok()
                            .and_then(|u| u.host_str().map(|h| h.to_string()))
                            .map(|host| {
                                input.allowed_domains.iter().any(|d| {
                                    host == *d || host.ends_with(&format!(".{}", d))
                                })
                            })
                            .unwrap_or(false)
                    })
                    .cloned()
                    .collect()
            } else {
                vec![]
            };

            for link in &outbound {
                if !visited.contains(link) {
                    queue.push_back((link.clone(), depth + 1));
                }
            }

            pages.push(CrawledPage {
                url: url.clone(),
                title,
                text,
                embedding,
                links: outbound,
                fetched_at: chrono::Utc::now()
                    .to_rfc3339_opts(chrono::SecondsFormat::Millis, true),
                content_hash,
            });
        }

        let total_text_chars: u64 = pages.iter().map(|p| p.text.len() as u64).sum();
        let total_fetched = pages.len() as u32;

        Ok(WebCrawlOutput {
            pages,
            total_fetched,
            total_text_chars,
            errors,
        })
    }
}
